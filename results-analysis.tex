% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Results Simulation Study},
  pdfauthor={Florian Stijven},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Results Simulation Study}
\author{Florian Stijven}
\date{2023-11-16}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

In this document, we present the results of the simulation study for
assessing the finite sample properties of the meta TCT methods. This simulation
study has three goals, ordered by importance:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The primary goal is to assess to what degree the theoretical asymptotic
  results translate to finite samples.
\item
  The secondary goal is to identify finite sample settings where the meta TCT
  methods are not trustworthy.
\item
  The tertiary goal is to assess the correctness of the implementation of the
  meta TCT methods in the \texttt{TCT} R-package.
\end{enumerate}

All simulated data sets were generated from multivariate normal distributions
matching the data generating model (DGM) used by Raket (2022, sec. 5.1).
This DGM represents a realistic 36-months clinical trial in
prodromal Alzheimer's disease. The control group is based on the analysis of a
selection\footnote{The following inclusion criteria were used by Raket (2022): ``at
  the baseline visit, patients must be diagnosed as having mild cognitive
  impairment, score less than or equal to 28 on the mini mental state examination
  (MMSE, range 30-0, higher scores indicate less impairment) and be amyloid
  positive according to a brain positron emission tomography scan or analysis of
  cerebrospinal fluid.''} of 556 patients from the Alzheimer's disease neuroimaging initiative (ADNI)
(Veitch et al. 2019).

We first present the DGM in more details together with a visualization of the
relevant mean trajectories. Second, the analysis methods we consider in this
simulation study are summarized. Finally, the simulation results are presented.

\hypertarget{data-generating-model}{%
\section{Data Generating Model}\label{data-generating-model}}

For the 556 patients selected from the ADNI,
the ADAS-cog scores were\footnote{13-item cognitive subscale of the Alzheimer's disease assessment scale,
  lower scores indicate less impairment (Rosen, Mohs, and Davis 1984)} available at baseline visits and 6, 12, 18, 24, and 36
months after baseline. The estimated means at the corresponding time points are
\[(19.6, 20.5, 20.9, 22.7, 23.8, 27.4)'\]
and the corresponding estimated covariance matrix is
\[\begin{pmatrix}
45.1 & 40.0 & 45.1 & 54.9 & 53.6 & 60.8 \\
40.0 & 57.8 & 54.4 & 66.3 & 64.1 & 74.7 \\
45.1 & 54.4 & 72.0 & 80.0 & 77.6 & 93.1 \\
54.9 & 66.3 & 80.0 & 109.8 & 99.3 & 121.7 \\
53.6 & 64.1 & 77.6 & 99.3 & 111.4 & 127.8 \\
60.8 & 74.7 & 93.1 & 121.7 & 127.8 & 191.4
\end{pmatrix}.\]
Patient-level data in the control group are generated from a multivariate normal
distribution with the above mean vector and covariance matrix, as in
Raket (2022, sec. 5.1). Whereas Raket (2022) considered
multiple types of treatment effects, we only consider treatment effects that
correspond to proportional slowing.

To simulate proportional slowing, we first consider the \emph{reference trajectory}.
Let \(Y_t\) be the ADAS-cog score \(t\) months after randomization and \(Z = 0\)
denote the control group. The \emph{reference trajectory} is then the mean ADAS-cog
score in the control group as a function of time since randomization, \[E(Y_{t}
| Z = 0) = f_{0}(t; \boldsymbol{\alpha})\] where \(\boldsymbol{\alpha}\) is a
parameter vector indexing the reference trajectory. This trajectory should be a
continuous function over the relevant range. In our DGM, this is \([0, 36]\) which
is the total duration of the ADNI study. From this same study, we only have 6
mean estimates at 6 distinct time points. Therefore, we interpolate between
these 6 points with natural cubic spline interpolation. This \emph{interpolated}
reference trajectory is plotted in Figure
\ref{fig:data-generating-model-visualization}.

For simulating data for the treated group (\(Z = 1\)), we consider trajectories of the
following form, \[E(Y_t | Z = 1) = f_0(\gamma \cdot t; \boldsymbol{\alpha}) \]
where \(\gamma\) is the \emph{acceleration factor}. These data are simulated from
a multivariate normal distribution with the means determined by the above
trajectory function and with the same covariance matrix as in the control group.

We consider a set of DGMs where the following elements are varied to represent
a range of realistic scenarios:

\begin{itemize}
\tightlist
\item
  \textbf{Progression Rate}. We consider a \emph{normal} and \emph{fast} progression rate. The
  normal progression rate corresponds to the mean vector presented above. For the
  fast progression rate, we change the mean vector in the control group to
  \[(18.0, 19.7, 20.9, 22.7, 24.7, 29.2)'.\] Alternatively, the fast progression
  scenario could also be interpreted as corresponding to trials where patients have
  been followed up longer.
\item
  \textbf{Treatment Effect}. We consider 3 different (proportional slowing) treatment
  effects, that is, \(\gamma \in \{1, 0.90, 0.75, 0.50 \}\).
\item
  \textbf{Sample Size}. We consider 4 different total sample sizes, \(n \in \{50, 200, 500, 1000\}\).
  Note that \(n\) is the \emph{total} sample size, and we assume \(1:1\) randomization in all settings.
\item
  \textbf{Duration of follow up}. We consider settings with 24 and 36 months of follow up,
  corresponding to 5 and 6 measurements of the ADAScog score, respectively.
\end{itemize}

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/data-generating-model-visualization-1.pdf}
\caption{\label{fig:data-generating-model-visualization}Plot of the trajectories used in the DGMs. A acceleration factor equal to 1 corresponds to no treatment effect. The dots correspond to measurement occasions while the lines between dots are based on interpolation.}
\end{figure}

\hypertarget{analysis-methods}{%
\section{Analysis Methods}\label{analysis-methods}}

In this section, two analysis methods are briefly outlined. We first present the
mixed model for repeated measures (MMRM) that is fitted to the simulated data
sets. Next, we discuss the non-linear generalized least squares (NL-GLS) version
of meta TCT. While we consider other versions of meta TCT as well, we will focus
on the NL-GLS version.

\hypertarget{mixed-model-for-repeated-measures}{%
\subsection{Mixed Model for Repeated Measures}\label{mixed-model-for-repeated-measures}}

For each simulated data set, a MMRM is fitted. This is a linear mixed model
where time is treated as a categorical covariate. The \textbf{systematic part} consists
of the interaction between treatment and time, except for \(t = 0\) where we
assume that the mean outcome is equal in both treatment groups. Let \(j = 0, 1, ..., K\)
denote the measurement occasions, and \(t_j\) the corresponding months after baseline.
The mean outcome is then modeled as
\[E(Y_{t_j} | Z = z) = \begin{cases}
 \alpha_j \text{ if } z = 0\\
 \beta_j \text{ if } z = 1
\end{cases}
\; \forall \; j \in \{ 1, ..., K \}\]
and \(E(Y_{0} | Z = 0) = E(Y_{0} | Z = 1) = \alpha_0\).
This essentially means that we have a parameter for each measurement
occasion-treatment combination, except for baseline. The \textbf{covariance matrix}
is assumed to be unstructured, but common for both treatment groups. For all
simulation scenarios, this is a correctly specified model.

These models are fitted with restricted maximum likelihood (REML) by the
\texttt{mmrm()} function from the \texttt{mmrm} R-package (Sabanes Bove et al. 2022). The parameter
estimates and corresponding variance-covariance matrix are obtained by the
\texttt{coef()} and \texttt{vcov()} methods, respectively. These two components are the only inputs required
for the meta TCT methods. The hypothesis test for
\[H_0: \alpha_j = \beta_j \; \forall \; j \in \{ 1, ..., K \} \]
is based on the F-test with the Kenward-Roger approximate degrees of freedom.

\hypertarget{meta-tct}{%
\subsection{Meta TCT}\label{meta-tct}}

The meta TCT methodology starts from the assumption that the estimator for the
mean vector has a multivariate normal sampling distribution,
\[(\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}})' = (\hat{\alpha}_{0}, ..., \hat{\alpha}_{K}, \hat{\beta}_{1}, ..., \hat{\beta}_{K})' \sim N((\boldsymbol{\alpha_0}', \boldsymbol{\beta_0}')', D).\]
In principle, we can replace the mean with any other functional of the
distribution such as the median, as long as the sampling distribution of the
estimator is (approximately) multivariate normal.

In the NL-GLS version of meta TCT, we treat this estimated vector, \((\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}})'\),
as the ``observed data'' and model these data with the following non-linear model,
\[ \begin{split}
    E\left( (\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}})' \right) & = \left( \boldsymbol{f_0}(\boldsymbol{t}; \boldsymbol{\alpha}), \boldsymbol{f_0}(\gamma \cdot \boldsymbol{t}; \boldsymbol{\alpha}) \right)' \\
    & = \left( \boldsymbol{\alpha},  \boldsymbol{f_0}(\gamma \cdot \boldsymbol{t}; \boldsymbol{\alpha}) \right)'
\end{split} \]
where \(\boldsymbol{f_0}(\gamma \cdot \boldsymbol{t}; \boldsymbol{\alpha}) = (f_0(\gamma \cdot t_1; \boldsymbol{\alpha}), ..., f_0(\gamma \cdot t_K; \boldsymbol{\alpha}))'\).
The second equality is a consequence of \(f_0\) being an interpolating function.
This regression function is fitted using non-linear generalized least squares.
Hence, the estimator for \((\boldsymbol{\alpha_0}, \gamma_0)'\) minimizes the generalized least squares criterion,
\[
    (\hat{\boldsymbol{\alpha}}^*, \hat{\gamma})' = \arg \min_{(\boldsymbol{\alpha}, \gamma)'} = \left( \begin{pmatrix}
        \hat{\boldsymbol{\alpha}} \\
        \hat{\boldsymbol{\beta}}
    \end{pmatrix} - \begin{pmatrix}
        \boldsymbol{\alpha} \\
        \boldsymbol{f_0}(\gamma \cdot \boldsymbol{t}; \boldsymbol{\alpha})
    \end{pmatrix} \right)' D^{-1} \left( \begin{pmatrix}
        \hat{\boldsymbol{\alpha}} \\
        \hat{\boldsymbol{\beta}}
    \end{pmatrix} - \begin{pmatrix}
        \boldsymbol{\alpha} \\
        \boldsymbol{f_0}(\gamma \cdot \boldsymbol{t}; \boldsymbol{\alpha})
    \end{pmatrix}  \right).
\]
In practice, we replace \(D\) with its estimate, \(\hat{D}\). As part of this
procedure, \(\boldsymbol{\alpha_0}\) is re-estimated by
\(\hat{\boldsymbol{\alpha}}^*\). However, we do not use
\(\hat{\boldsymbol{\alpha}}^*\) further on.

Inference is based on the generalized least-squares criterion. Let \(l_1\) be the
minimized generalized least-squares criterion under no restrictions, and \(l_0\)
the criterion under the restriction that \(\gamma = \gamma^*\). It then follows
that \(l_1 - l_0 \, \dot\sim \, \chi^2_1\) under the null that \(\gamma_0 = \gamma^*\). Note that this result is asymptotic with respect to \(n \to \infty\)
while the length of \((\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}})'\)
remains constant.

The standard errors for the estimates from the above approach can be obtained
analytically (by linear approximation) or through a parametric bootstrap. In the
remainder of this report, the standard error is estimated analytically unless
mentioned otherwise.

\hypertarget{results}{%
\section{Results}\label{results}}

In this section, the results of the simulation study are presented. For each
setting, we consider \(5000\) replications. For estimating the empirical coverage
of the 95\% CIs and the empirical type 1 errors, this leads to a standard error of
\(\frac{\sqrt{0.05 \cdot 0.95}}{\sqrt{5000}} = 0.003\) assuming nominal coverage
and a nominal type 1 error.

In this section, we only present the results of the NL-GLS version of meta TCT
that uses all measurement occasions. In the Appendix, the results for other
estimators are presented in an interactive table.

The analyses and the presentation of the simulation results are divided into two
parts. First, we present the results regarding estimation of the acceleration
factor. This is the primary goal of the meta TCT methods: Transforming the
treatment effect on a difficult to interpret scale to the time scale. Second, we
present the results regarding inference, that is, hypothesis tests and
confidence intervals. The operating characteristics of the meta TCT methods are
also compared with those of the MMRMs.

\hypertarget{estimation}{%
\subsection{Estimation}\label{estimation}}

To evaluate estimation, we look at 3 key performance measures,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Bias of the estimator.} We estimate \(E(\hat{\gamma})\)
  across different settings and compare the estimated expectation with the
  true value, \(\gamma_0\).
\item
  \textbf{Mean squared error (MSE).} The MSE is defined as
  \(E\{(\hat{\gamma} - \gamma_0)^2 \}\). This quantity is estimated as the mean of the
  squared differences between \(\hat{\gamma}\) and \(\gamma_0\). This quantity quantifies the average
  distance between the estimator and the estimand, which depends on the variance and
  bias.
\item
  \textbf{Empirical standard deviation.} The empirical standard deviation of the estimator simply
  is the standard deviation of the estimator. This measure is estimated as the sample
  standard deviation of the estimates in each setting. This value is compared with the
  median \emph{estimated standard error}.
\end{enumerate}

In Figure \ref{fig:expected-value-estimator}, the mean of the estimated
acceleration factors is presented across a set of scenarios. In all but one
scenario, the bias decreases to almost 0 as the total sample size increases to
\(1000\). This does not hold up only for the scenario with \(\gamma = 0.90\), normal
progression and 36 months of follow up. A possible explanation is that - looking
at Figure \ref{fig:data-generating-model-visualization} - the estimated mean at
36 months in the experimental group is mapped into a time interval with no
measurement occasions nearby. This time mapping is therefore rather sensitive
to the interpolation method. In contrast, time mappings into the \([0, 18]\)
interval are expected to be less sensitive to the interpolation approach. This
also explains why there is no bias is the corresponding scenario with only 18 months
of follow up.

In any case, the bias is generally
smaller than 0.05. There is only one scenario with a larger bias: \(\gamma = 0.5\), normal progression and 24 months of follow up.

Figure \ref{fig:expected-value-estimator} also shows that the bias is
considerably smaller for the fast progression rate, as compared to the normal
progression rate. The bias also tends to be smaller when comparing 24 months of
follow up with 36 months, but this is not generally the case.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/expected-value-estimator-1.pdf}
\caption{\label{fig:expected-value-estimator}Graphs of the means of the estimated acceleration factors across a set of simulation settings. The presented results are based on the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor. In each subplot, a black horizontal line represents the true acceleration factor. The maximum standard error of the mean stratified by sample size is: \(n = 50\); \(0.027\), \(n = 200\), 0.003; \(n = 500\), 0.002; and \(n = 1000\), 0.001.}
\end{figure}

In Figure \ref{fig:mse-estimator}, the MSE of the estimator for the common
acceleration factor is presented across the same set of scenarios as before. As
expected, the MSE decreases as a function of the sample size. Moreover, a longer
follow up and faster progression lead to a smaller MSE.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/mse-estimator-1.pdf}
\caption{\label{fig:mse-estimator}Graphs of the MSEs for the estimator of the common accleration factor across a set of simulation settings. The presented results are based on the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor. Note that both axes are log10-transformed.}
\end{figure}

In Figure \ref{fig:se-estimator}, the empirical standard deviations are plotted
together with the median estimated standard errors. For small sample sizes, the
standard error estimators underestimate the empirical standard error. However,
this underestimation largely disappears for larger sample sizes. As for the MSE,
Figure \ref{fig:se-estimator} shows that a longer follow up and faster
progression lead to a smaller empirical standard deviation.

Although inference is not based directly on the estimated standard error, the
observed
\emph{underestimation} for smaller sample sizes would lead to issues if the estimated acceleration factor were
used in meta-analyses. We have therefore also implemented a bootstrap-based
estimator for the standard error, as presented in the next section.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/se-estimator-1.pdf}
\caption{\label{fig:se-estimator}Graphs of the empirical standard deviations and the median estimated standard errors of the estimator for the common acceleration factor across a set of simulation settings. The dots, and connecting lines, represent the empirical standard deviations and the triangles represent the median estimated standard errors. The presented results are based on the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor while the columns correspond to the duration of follow-up. Note that both axes are log10-transformed.}
\end{figure}

\hypertarget{inference}{%
\subsection{Inference}\label{inference}}

To evaluate inference, we look at 2 key performance measures,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Type 1 error and power.} We compare the empirical type 1 error with the
  nominal rate, \(\alpha = 0.05\). We do this for the meta TCT methods as well as for
  the MMRMs. The former is based on the latter, so we also expect that discrepancies
  between empirical and nominal type 1 error rates for the MMRM will be reflected in
  the meta TCT methods.
\item
  \textbf{Coverage.} We asses the empirical coverage rate of the estimated \(95\%\)
  CIs.
\end{enumerate}

In Figure \ref{fig:error-rates-meta-tct}, we graph the empirical type 1 error
rate and power. The corresponding empirical operating characteristics for the
F-test in the MMRMs are superimposed in gray. This reveals that the type 1 error
rate for meta TCT is inflated for the sample sizes between 50 and 500.
Consequently, the power for the corresponding settings is not well-calibrated.
For the settings with \(n = 1000\), the empirical type 1 error is close to
nominal, hence, the corresponding empirical powers are well-calibrated and can
be interpreted as usual. In these scenarios, the power of the meta TCT test is
larger than or equal to the power of the corresponding F-test in the MMRM. A
longer follow up and faster progression also lead to a larger power, which is
consistent with the previous results.

The tests presented in Figure \ref{fig:error-rates-meta-tct} are asymptotic.
So, poor performance in small samples is not surprising. To provide robust
inference for small samples, we also assess the performance of a parametric
bootstrap in the next section.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/error-rates-meta-tct-1.pdf}
\caption{\label{fig:error-rates-meta-tct}Graphs of the empirical type 1 error rate and power across a set of simulation settings. The presented results are based on the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor while the columns correspond to the duration of follow-up.}
\end{figure}

In Figure \ref{fig:coverage-meta-tct}, the empirical coverage rates are
presented for the same settings as before. This reveals that there is
undercoverage for the sample sizes between 50 and 500. This undercoverage
decreases with an increasing sample size, but does not disappear completely for
some scenarios with a sample size of 1000.

There is one scenario where the coverage does not converge to 95\% for an
increasing sample size: the scenario with \(\gamma = 0.90\), normal progression
and 36 months of follow up. This is also the scenario where the bias was not
zero for a sample size of 1000. The explanation for this bias also explains the
observed undercoverage.

As mentioned before, the coverage of alternative confidence intervals based on
a parametric bootstrap is assessed in the next section.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/coverage-meta-tct-1.pdf}
\caption{\label{fig:coverage-meta-tct}Graphs of the empirical coverage across a set of simulation settings. The presented results are based on the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor while the columns correspond to the duration of follow-up.}
\end{figure}

\hypertarget{parametric-bootstrap}{%
\section{Parametric Bootstrap}\label{parametric-bootstrap}}

In small samples, the operating characteristics of the NL-GLS meta TCT estimator
deviate considerably from nominal. This is not surprising because the
corresponding inferential procedures are only asymptotic. We provide an alternative
inferential procedure based on a parametric bootstrap. The idea behind this
parametric bootstrap is to resample the estimated parameters from the MMRM from
the estimated multivariate normal sampling distribution, that is,
\[(\hat{\boldsymbol{\alpha}}^{b}, \hat{\boldsymbol{\beta}}^{b})' \, \sim \, N\left( (\hat{\boldsymbol{\alpha}}', \hat{\boldsymbol{\beta}}')', \hat{D}  \right)\]
where

\begin{itemize}
\tightlist
\item
  \((\hat{\boldsymbol{\alpha}}', \hat{\boldsymbol{\beta}}')'\) is the estimated
  mean vector.
\item
  \(\hat{D}\) is the estimated variance-covariance matrix of the sampling
  distribution.
\item
  \((\hat{\boldsymbol{\alpha}}^{b}, \hat{\boldsymbol{\beta}}^{b})'\) is the \(b\)'th
  bootstrap replicate of the mean vector.
\end{itemize}

The \(b\)'th bootstrap replicate of the common acceleration factor,
\(\hat{\gamma}^b\), is obtained by applying NL-GLS meta TCT to
\((\hat{\boldsymbol{\alpha}}^{b}, \hat{\boldsymbol{\beta}}^{b})'\) and
\(\hat{D}\). Subsequent inference is based on the \(1 - \alpha\) percentile
confidence interval, that is, \[(\hat{\gamma}^b_{\alpha / 2}, \hat{\gamma}^b_{1
- \alpha / 2})\] where \(\hat{\gamma}^b_{p}\) is the \(p\)-th percentile of the
bootstrap distribution.

To limit the computational burden, we only use \(B = 500\) bootstrap replications
throughout. We also only consider the bootstrap for the normal progression
scenarios because the largest deviations from nominal were observed there.

In Figure \ref{fig:se-estimator-bootstrap}, the empirical standard deviations
are plotted together with the median estimated standard errors that are based on
the parametric bootstrap. The median estimated standard errors now closely match
the empirical standard deviation. Only for a very small sample size, \(n = 50\),
is there some overestimation of the empirical standard deviation. So, the
parametric bootstrap provides a standard error estimator that is valid even in
small samples. However, some caution is warranted in very small sample sizes.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/se-estimator-bootstrap-1.pdf}
\caption{\label{fig:se-estimator-bootstrap}Graphs of the empirical standard deviations and the median estimated standard errors of the estimator for the common acceleration factor. The standard errors are estimated thrrough the parametric bootstrap as explained in the text. The dots, and connecting lines, represent the empirical standard deviations and the triangles represent the median estimated standard errors. The presented results are based on the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor while the columns correspond to the duration of follow-up. Note that both axes are log10-transformed.}
\end{figure}

In Figure \ref{fig:coverage-bootstrap}, the empirical coverage rates are
presented for the percentile confidence intervals. This reveals that there is
overcoverage which disappears with an increasing sample size. For \(n = 1000\),
coverage is nominal, except in the scenario with \(\gamma = 0.90\), normal
progression and 36 months of follow up. This is, again, the same scenario where
we observed bias and non-nominal coverage for the standard CIs. So, the same
explanation for the observed deviation from nominal holds here.

These results thus indicate that the parametric bootstrap permits valid, but generally
conservative, inference in small samples.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/coverage-bootstrap-1.pdf}
\caption{\label{fig:coverage-bootstrap}Graphs of the empirical coverage of the percentile bootstrap confidence intervals for the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor.}
\end{figure}

Finally, the empirical type 1 error rates and power for the parametric bootstrap
are presented in Figure \ref{fig:error-rates-bootstrap}. The corresponding
hypothesis tests are based on the percentile confidence intervals. First, the
empirical type 1 error rate tends be be conservative for all sample sizes, but
converges to nominal for an increasing sample size. Second, the empirical power
for meta TCT is generally similar to the power of the F-tests in the MMRMs. However,
there are settings where the meta TCT is more powerful. For instance, for
\(\gamma = 0.5\) and \(n = 50\), the power of the meta TCT method is considerably
larger than the power of the F-test.

\begin{figure}
\centering
\includegraphics{results-analysis_files/figure-latex/error-rates-bootstrap-1.pdf}
\caption{\label{fig:error-rates-bootstrap}Graphs of the empirical type 1 error rate and power across a set of simulation settings. The hypothesis tests are based on the percentile confidence intervals for the NL-GLS version of meta TCT. The rows correspond to the true acceleration factor.}
\end{figure}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

The results are summarized according to the goals of the simulation study. We
only looked a the results with the NL-GLS version of meta TCT that uses all
measurement occasions. A detailed analysis of the results with other versions
of meta TCT is outside the scope of this report.

\emph{Primary goal: Evaluate finite sample properties of the method, also in relation
to asymptotic inferential procedures.}

\begin{itemize}
\tightlist
\item
  \textbf{Bias}

  \begin{itemize}
  \tightlist
  \item
    For small sample sizes, the estimator for the acceleration factor is in some
    settings slightly biased. However, this bias is generally within 0.05 units of
    zero and goes to zero as the sample size increases to 1000.
  \item
    If there are time mappings to time points which do not have measurement occasions nearby,
    the estimator could remain biased, even for a sample size of 1000. This is
    explained by the fact that such time mappings heavily rely on the
    interpolation approach. The interpolated function may indeed be unstable for time
    points with no observed points nearby.

    \begin{itemize}
    \tightlist
    \item
      In these settings, similar issues are observed with the empirical coverage of
      the confidence intervals.
    \end{itemize}
  \item
    The bias also generally decreases with

    \begin{itemize}
    \tightlist
    \item
      an increasing sample size,
    \item
      an increasing duration of follow up,
    \item
      and an increasing progression rate.
    \end{itemize}
  \end{itemize}
\item
  \textbf{MSE}

  \begin{itemize}
  \tightlist
  \item
    The MSE decreases with

    \begin{itemize}
    \tightlist
    \item
      an increasing sample size,
    \item
      an increasing duration of follow up,
    \item
      and an increasing progression rate.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Estimated SE}

  \begin{itemize}
  \tightlist
  \item
    The analytic SE estimator underestimates the true SE for the small sample
    sizes. However, this underestimation largely disappears for a sample size of 1000.
  \item
    The parametric bootstrap SE estimator performs better than the
    analytic estimator. Only with a sample size of 50 are there some settings
    where the true SE is overestimated.

    \begin{itemize}
    \tightlist
    \item
      The availability of an estimator for the SE that is generally valid and
      applicable is important if the results of the meta TCT analysis were to be
      used in meta-analyses.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Confidence Intervals}

  \begin{itemize}
  \tightlist
  \item
    There is undercoverage for the confidence intervals based on the least-squares criterion
    in the smaller sample sizes. This undercoverage largely disappears for a sample
    size of 1000.
  \item
    There is overcoverage for the confidence intervals based on the parametric
    bootstrap in the smaller sample sizes. This overcoverage largely disappears for
    a sample size of 1000.
  \end{itemize}
\item
  \textbf{Type 1 Error and Power}

  \begin{itemize}
  \tightlist
  \item
    For the smaller sample sizes, the hypothesis test based on the least-squares
    criterion has an inflated type 1 error, whereas the tests based on the parametric
    bootstrap are conservative.

    \begin{itemize}
    \tightlist
    \item
      Tests based on the parametric bootstrap should thus be preferred for small
      sample sizes.
    \end{itemize}
  \item
    For a sample size of 1000, both types of tests have an empirical type 1 error
    that is close to nominal.

    \begin{itemize}
    \tightlist
    \item
      For this sample size, the power of the least-squares test is larger than
      the power of the bootstrap test and the F-test in the MMRM.
    \end{itemize}
  \item
    For certain settings with a small sample size, the bootstrap test
    outperforms the F-test in the MMRM.
  \end{itemize}
\end{itemize}

\emph{Secondary goal: Identify settings where the meta TCT methods may yield
unstable results.}

\begin{itemize}
\tightlist
\item
  \textbf{Sample size}

  \begin{itemize}
  \tightlist
  \item
    The meta TCT method is generally less stable in the scenarios with a sample size of 50.
    However, for a sample size of 200 or larger, the method is quite stable.

    \begin{itemize}
    \tightlist
    \item
      For inference in small samples, the parametric bootstrap method should be preferred.
    \item
      Stability also depends on many other factors besides sample size. For instance,

      \begin{itemize}
      \tightlist
      \item
        Whether the assumption of a multivariate normal sampling distribution holds for
        the given sample size. Most methods for analyzing longitudinal data only
        lead to a multivariate normal sampling distribution in an asymptotic sense.
        So, one should always be skeptical of this assumption. The parametric
        bootstrap also relies on this normality assumption and will thus not solve
        potential issues.
      \end{itemize}
    \end{itemize}
  \end{itemize}
\item
  \textbf{Duration of follow up}

  \begin{itemize}
  \tightlist
  \item
    A longer follow up generally leads to more stable results.
  \item
    There is, however, an important caveat. If there are time mappings to points
    in time with no measurement occasions nearby, the methods may become \emph{less stable}.
    For instance, if there are regular measurements in the first two years of the study,
    one should be careful with including a measurement 5 years post-randomization.
    If this measurement maps to, e.g., 3.5 years, this mapping will be very sensitive
    to the interpolation method and could lead to unstable results.
  \end{itemize}
\item
  \textbf{Progression rate}

  \begin{itemize}
  \tightlist
  \item
    A faster progression rate leads to more stable results. This is expected because
    faster progression translates to a steeper reference trajectory. It is furthermore
    easier to map estimated means to a steeper reference trajectory.
  \end{itemize}
\end{itemize}

\emph{Tertiary Goal: Evaluate correctness of the implementation in the \texttt{TCT}
R-package.}

\begin{itemize}
\tightlist
\item
  The functions implemented in \texttt{TCT} behaved as expected. For the NL-GLS version
  of meta TCT, we analyzed 320,000 simulated data sets without any failures. We can thus be confident that the methods implemented in the \texttt{TCT} R-package will not fail
  in similar data sets.
\end{itemize}

\hypertarget{appendix}{%
\section{Appendix}\label{appendix}}

\hypertarget{all-simulation-settings}{%
\subsection{All Simulation Settings}\label{all-simulation-settings}}

In the following table, we summarize the results for all simulation settings. In
the html version of the file, it is possible the interactively search through and
filter this table.

\begin{table}

\caption{\label{tab:unnamed-chunk-5}Summary of all simulation scenarios.}
\centering
\begin{tabular}[t]{}
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\\
\hline
\end{tabular}
\end{table}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-raket_progression_2022}{}}%
Raket, Lars Lau. 2022. {``Progression Models for Repeated Measures: {Estimating} Novel Treatment Effects in Progressive Diseases.''} \emph{Statistics in Medicine} 41 (28): 5537--57. \url{https://doi.org/10.1002/sim.9581}.

\leavevmode\vadjust pre{\hypertarget{ref-rosen1984new}{}}%
Rosen, Wilma G, Richard C Mohs, and Kenneth L Davis. 1984. {``A New Rating Scale for Alzheimer's Disease.''} \emph{The American Journal of Psychiatry} 141 (11): 1356--64.

\leavevmode\vadjust pre{\hypertarget{ref-mmrmpackage}{}}%
Sabanes Bove, Daniel, Julia Dedic, Doug Kelkhoff, Kevin Kunzmann, Brian Matthew Lang, Liming Li, and Ya Wang. 2022. \emph{Mmrm: Mixed Models for Repeated Measures}. \url{https://openpharma.github.io/mmrm/}.

\leavevmode\vadjust pre{\hypertarget{ref-veitch2019understanding}{}}%
Veitch, Dallas P, Michael W Weiner, Paul S Aisen, Laurel A Beckett, Nigel J Cairns, Robert C Green, Danielle Harvey, et al. 2019. {``Understanding Disease Progression and Improving Alzheimer's Disease Clinical Trials: Recent Highlights from the Alzheimer's Disease Neuroimaging Initiative.''} \emph{Alzheimer's \& Dementia} 15 (1): 106--52.

\end{CSLReferences}

\end{document}
